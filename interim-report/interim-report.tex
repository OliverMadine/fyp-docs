\documentclass{article}

\usepackage[margin=2cm]{geometry}
\usepackage{titling}
\usepackage[numbers]{natbib}
\usepackage{parskip}
\usepackage[hyphens]{url}

% Questions
% better to move challenges (1.3) and prior work (1.4) to background?

% Config
\bibliographystyle{unsrtnat}
\let\oldciteauthor\citeauthor
\renewcommand{\citeauthor}[1]{\oldciteauthor{#1} \cite{#1}}
\newcommand{\be}[1]{\textbf{\emph{#1}}}

% Title
\title{Fuzzing for Parser Combinators \\ {\large BEng Project Interim Report}}
\author{Oliver Madine}
\date{Janurary 21, 2024}

% Document
\begin{document}
\maketitle

\section{Introduction} % 1.5 pages

% 1. Why fuzzing for parsers is important?
\subsection{Motivation}
Parsers are critical components of many software systems with compilers being a stand out example. Issues in these systems can be devastating; recently, a bug in the Vyper compiler caused a security vulnerability for Curve Finance, reportedly costing up to \$62 million \cite{curve}. Extensive testing of parsing libraries is vital to both support overall correctness of these systems and establish user trust. Traditional software testing methods, however, can be expensive, accounting for up to 50\% of development costs \cite{quickcheck}.

% 2. In this work we do X.
\subsection{Project}
This project explores the application of various fuzzing techniques in the context of parser combinator libraries, an unexplored domain, with Gigaparsec \cite{Gigaparsec} as the case study. Although many techniques are considered, the primary method investigated involves synthesising arbitrary parsers with corresponding inputs and using metamorphic fuzzing \cite{metamorphic} to test adherence to the parser combinator laws outlined by \citeauthor{parsley}.

\subsection{Challenges}
% 3. interesting / challenges

\subsubsection{Sample Generation}
When executing a test case for a parser, failing to parse an input token may cause the parser to quickly return. This limits both the statement coverage and behavioural validation of the test, therefore, both the rejection and acceptance states of a parser should be tested. Arbitrary inputs are likely to cause substantially more rejections than acceptances. A significant challenge of the project is to generate valid inputs for a given parser. 

\subsubsection{Left Recursion}
Left recursive grammars are grammars that have at least one non-terminal element eventually deriving to a form with itself as the left-most symbol \cite{left-recursive}. These grammars can be handled with chain combinators \cite{design-patterns}, but naive parser compositions can still lead to infinite recursion. In general, parsers that invoke themselves without consuming any input will not terminate, and thus cannot be tested and will be problematic if randomly generated.

% 4. Previous work on X has addressed these with Y, but the problems with this are Z.
\subsection{Prior Work}

\subsubsection{Sample Generation}

Several techniques of sample generation have been explored in the context of parsers. 

\citeauthor{parser-directed} explores parser-directed fuzzing by using dynamic tainting to resolve parse rejections. This approach relies on a direct taint, which is often ineffective during tokenization stages of parsers when the data flow is broken, limiting the exploration of the input space.

Symbolic execution for fuzzing tackles the sample generation challenge by using constraint solving to increases statement coverage \cite{klee}. Path explosion is particularly problematic in the context of parsers as paths predominantly cause parse rejections \cite{path-explosion}. Samples that are accepted by the parser remain highly improbable \cite{parser-directed}.

Grammar-based fuzzing, which involves generating samples directly from a formal grammar, avoids much of the trial and error of random input generation for parsers \cite{grammar}. Traditionally, grammars are difficult to write and maintain, often do not reflect the actual grammar implemented by a parser, and are not necessarily machine readable.

\subsubsection{Left Recursion}
Although there has been some work towards designing parser combinators libraries that terminate with left-recursive structures \cite{left-recursive-detect}, these libraries currently face performance limitations and are not widely adopted. There is no prior work towards exploring the random generation of terminating parsers. 

% 5) This has the following appealing properties and our experiments show this and that.
\subsection{Unique Benefits}
Parser combinator libraries are generally Domain-Specific Languages (DSLs) embedded into functional host languages, resulting in strongly-typed APIs \cite{monadic-combinators, parsec}. This highlights the first uniquely interesting advantage of the project: the specification of valid parsers is defined directly through the parser typing, allowing well-defined parsers to be generated randomly.

The second novel aspect of this approach is that parser combinator libraries embed the target grammar into the host language \cite{combinator-parsing}. The structure of the generated parsers can be inspected to guide valid input generation for the parsers.

Considering these benefits, we propose that fuzzing for parser combinators will systematically verify parser combinator libraries by synthesising new parsers and effectively covering their input spaces.

% grading criteria: analysis, critical judgement, generalising common aspects, synthesis / organised, alternative approaches 
\section{Background} % 10 pages

\subsection{Parser Combinators}
% Parser Combinator libraries are an increasingly popular method of writing parsers by treating parsers as first class values to form larger parsers by combining smaller parsers.
\subsubsection{Parsec}
\subsubsection{Parsec}
% parsley
\subsubsection{Gigaparsec}
\subsubsection{Laws}

\subsection{Fuzzing}
\subsubsection{Metamorphic Testing}
\subsubsection{Differential Testing}
%     - differential against parsley: unlikely to catch unconsidered edge case: 
%     - mutation based on laws: we must find and verify these laws
%     - differential testing on optimisations: cannot find bugs in unoptimised Gigaparsec. Gigaparsec is yet to implement optimisations
\subsubsection{QuickCheck}
\subsubsection{Quickspec}

\subsection{Summary}

\section{Evaluation Plan} % 1 page

The project is evaluated using several metrics. It is important that experiments are carried out carefully using multiple trials, statistical tests, and considering a variety of random seeds to avoid result bias \cite{evaluation}.

\subsection{Unique Bugs}

When evaluating the total bugs discovered during the experiments, it is important that duplication is considered. For example, a bug discovered in one parser, may also be discovered in similar parsers.

Deduplication via reduction of the generated parsers and test samples would be desirable to report as many distinct bugs as possible between versions \cite{deduplication}. However, this is a non-trivial problem and is outside the scope of the project. Some bug deduplication may be possible manually, otherwise, bug duplication will be a limitation of the evaluation.

\subsection{Practical Significance of Bugs}

We hypothesize that any Gigaparsec bugs discovered have a high likelihood of being relevant in practical applications. \citeauthor{fuzzing-importance} demonstrates that similar is true for compiler fuzzing.

However, it could still be argued that many bugs discovered in this project are niche cases and unlikely to be found through realistic use of the library. A formal evaluation of the practical significance of any bugs discovered would be interesting to explore, but this may require the analysis of a many real-world Gigaparsec parsers and is thus outside of the scope of the project.

\subsection{Bug Injection}

Existing fuzzing tools cannot be used for comparison since the generation of arbitrary parsers are entirely custom. Regardless, evaluating ability of the tool to discover injected bugs will allow us to validate the thoroughness of the fuzzing approach. We can use old bugs or mutation when injecting new bugs.

\subsection{Statement Coverage}

Statement coverage is a common evaluation metric for fuzzing. \citeauthor{coverage} argue that statement coverage is a poor indicator of test suite quality. Statement coverage does not necessarily account for the complexity of the interaction between statements, which may be especially true for parser combinators, where the interactions between combinators can be intricate. For these reasons, statement coverage is not a primary evaluation metric for this project.

\subsection{Qualitative Measures}

Qualitatively, the implementation should be well integrated into the Gigaparsec library, easy to use with future version, be well documented, and have detailed execution logs.

\section{Project Plan} % 1 page

As for my personal schedule, I am working full-time during the project and will have consistent free time (or lack thereof) from month to month. As a result, the project workload is distributed evenly.

\subsection{Milestone 1: Parser Generation}
\be{February, 2024}

The first milestone is to implement generators for some of the primitive combinators in Gigaparsec using QuickCheck \cite{quickcheck}. Some progress has been made towards this goal, but more work is required to support additional generators, implement size bounds for the generated parsers, and the consider the distribution of generated parsers.

\subsection{Milestone 2: Input Generation}
\be{March, 2024}

The second milestone is to implement input generation for the parsers themselves by inspecting the structure of arbitrary parsers. High coverage of the input space is important here, ideally with many inputs being accepted by the parser to test both acceptance and rejection states.

\subsection{Milestone 3: Metamorphic Testing}
\be{April, 2024}

In the third milestone, parser combinators laws will be used find equivalent parsers for generated parsers, which are then tested against each other to check adherence to the laws. Apply transformations using laws with specified expected changes in behaviour is another possible form of metamorphic testing to be explored.

\subsection{Milestone 4: Experiments, Evaluation, and Report}
\be{June 17, 2024}

This milestone has been allocated the largest amount of time, partially as a buffer for the previous milestones, and partially as there some uncertainty surrounding the time required for the final milestone. Both the execution time for the fuzz testing and and the hardware available are unclear. It may also be necessary to implement test case reduction to help with the interpretation of the results. The final report will be written in parallel alongside the experiments for this milestone.

\subsection{Extensions}

If time permits, there are a several areas for extension. Both the combinators and combinator laws could be extended to test Gigaparsec more thoroughly, alternatively, other fuzzing techniques could be explored to evaluate their potential for future research.

Differential testing of Gigaparsec and Parsley \cite{garnishing} would be particularly interesting to explore. The two libraries are intended to be API compatible, but have significantly different implementations due to the differences in the host languages (Haskell and Scale respectively). Consequently, it seems plausible that there would be different sets of bugs in each library, leaving opportunity for differential testing. Parsley is also much more mature that Gigaparsec, meaning it could be used as a reference to support the stability of Gigaparsec throughout its early development and optimisation.

\subsection{Fallback Plan}
Each milestone is designed to be flexible in its totality, allowing for easy adaption to meet project deadlines. For example, in the first milestone, the number of supported combinators is variable. The second milestone is the least flexible; good coverage of the input space is required for the supported parsers. For the third milestone, the parser laws tested can be reduced. For the final milestone, the execution time for the experiments can be reduced at the cost of reduced coverage.

\raggedright
\bibliography{interim-report.bib}
\end{document}

% Similarly to other successful parser-direct fuzzing approach, the parser must be exposed to the fuzzer for our implementation.