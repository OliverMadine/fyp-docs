\documentclass[11pt]{article}

\usepackage{geometry}
\usepackage{titling}
\usepackage[numbers]{natbib}
\usepackage{parskip}
\usepackage[hyphens]{url}

% Config
\bibliographystyle{unsrtnat}
\let\oldciteauthor\citeauthor
\renewcommand{\citeauthor}[1]{\oldciteauthor{#1} \cite{#1}}
\newcommand{\be}[1]{\textbf{\emph{#1}}}

% Title
\title{Fuzzing for Parser Combinators \\ {\large BEng Project Interim Report}}
\author{Oliver Madine}
\date{Janurary 21, 2024}

% Document
\begin{document}
\maketitle

\section{Introduction}

% 1. Why fuzzing for parsers is important?
\subsection{Motivation}
Parsers are critical components of many software systems with compilers being a stand out example. Issues in these systems can be devastating; recently, a bug in the Vyper compiler caused a security vulnerability for Curve Finance, reportedly costing up to \$62 million \cite{curve}. Extensive testing of parsing libraries is vital to both support overall correctness of these systems and establish user trust. Traditional software testing methods, however, can be expensive, accounting for up to 50\% of development costs \cite{quickcheck}.

% 2. In this work we do X.
\subsection{Project}
This project explores the application of fuzzing in the context of parser combinator libraries, an unexplored domain, with Gigaparsec \cite{gigaparsec} as the case study. Although many techniques are considered, the primary method investigated is metamorphic fuzzing \cite{metamorphic}. We aim to synthesise arbitrary parsers with corresponding valid inputs and test their adherence to the parser combinator laws outlined by \citeauthor{parsley}.

% 3. interesting / challenges
\subsection{Challenges}

\subsubsection{Sample Generation}
When executing a test case for a parser, failing to parse an input token may cause the parser to quickly return. This limits both the statement coverage and behavioural validation of the test, therefore, both the rejection and acceptance states of a parser should be tested. Arbitrary inputs are likely to cause substantially more rejections than acceptances. A significant challenge of the project is to generate valid inputs for a given parser. 

\subsubsection{Non-Terminating Parsers}
Some parser compositions can infinitely recurse thus cannot be tested and will be problematic if randomly generated by our fuzzer. We must ensure that the generated parsers are guaranteed to terminate.

% 5) This has the following appealing properties and our experiments show this and that.
\subsection{Unique Benefits}
Parser combinator libraries are Domain-Specific Languages (DSLs) generally embedded into functional host languages, resulting in strongly-typed APIs \cite{monadic-combinators, parsec}. This highlights the first uniquely interesting advantage of the project: the specification of valid parsers is defined directly through the parser typing, allowing well-defined parsers to be generated randomly.

The second novel aspect of this approach is that parser combinator libraries embed the target grammar into the structure of the parser \cite{combinator-parsing}. An arbitrary parser can be inspected to guide the generation of valid input samples.

Considering these benefits, we propose that fuzzing for parser combinators will effectively verify parser combinator libraries by generating new parsers and efficiently covering their input spaces.

% grading criteria: analysis, critical judgement, generalising common aspects, synthesis / organised, alternative approaches 
\section{Background}

\subsection{Parsers}
Parsers in read characters and build a more structured representation of the input. They are often used to validate the syntax of a program in a compiler, but can be used to extract data for any system with unstructured input.

\subsubsection{Parser Combinators}
Parser Combinator libraries are an increasingly popular method of writing parsers by treating parsers as first class values to form larger parsers by combining smaller ones \cite{parsley}. They allows parsers to follow the structure of an underlying grammar \cite{efficient-combinators}, are modular in nature \cite{efficient-combinators}, and do not require any additional tooling as they are directly embedded into the host language \cite{parsec}.

\subsubsection{Grammars}
Context free grammars are grammars formed from a set of recursive rules, where each rule is a non-terminal that produces a sequence of terminal and/or non-terminal symbols \cite{context}. In a context free grammar, a production rule can be applied regardless of the context. Context free grammars can be parsed without maintaining any context throughout the parsing the prior symbols \cite{parsley}.

In a context sensitive grammar (CFG), a the previously parsed symbols may be considered when applying a production rule \cite{context}. Context sensitive grammars require the parser to maintain a context throughout the parsing of the input \cite{parsley}. Monadic parser combinators can handle CFGs by passing some context through the combinators \cite{parsley}.

\subsubsection{Parsec}
Parsec \cite{parsec} is a popular monadic parser combinator library for Haskell that avoids space leaks and improves on the error messages of previous work. It was one of the earlier parser combinator libraries and has since been widely adopted, however, it historically suffers in performance when compared with parser generators \cite{staged-selective}.

\subsubsection{Parsley}
Parsley is a mature parsec-style parser combinator library written in Scala \cite{parsley}. It is highly optimised through the use of a deep embedding DSL and staging \cite{staged-selective}. 

\subsubsection{Gigaparsec}
Gigaparsec is a more recent parsec-style parser combinator library for Haskell, aiming towards an approachable API (avoiding template-haskell) while still being more efficient than Parsec \cite{gigaparsec}. Gigaparsec aims to be API compatible with Scala Parsley.

\subsubsection{Left Recursion}
Left recursive grammars are grammars that have at least one non-terminal element eventually deriving to a form with itself as the left-most symbol \cite{left-recursive}. These grammars can be handled with chain combinators \cite{design-patterns}, but naive parser compositions can still lead to infinite recursion. In general, parsers that invoke themselves without consuming any input will not terminate.

Although there has been some work towards designing parser combinators libraries that terminate with left-recursive structures \cite{left-recursive-detect}, these libraries currently face performance limitations (due to the expansion of left-recursive non-terminals) and are not widely adopted. For these reasons, the validation of parser combinator libraries that allow infinite left recursion remains a valuable problem to solve.

\subsubsection{Laws}

Some research has been done towards formalising the behaviour of parser combinators. \citeauthor{parsley} defines various categories of parser combinators with corresponding combinator laws. \citeauthor{staged-selective} extends this work with laws specific to primitive combinators, and some derived from laws of propositional logic. Although, this set of laws is not necessarily minimal, each is useful in the scope of this project. Further work to formalise more laws in the context of parser combinator primitives could be done.

\subsection{Fuzzing}
Fuzzing is a software testing technique that involves using randomly generated inputs to a program to test for unexpected behaviours \cite{fuzzing-importance}. Fuzzing is a popular technique for testing compilers, but has not yet been applied to parser combinator libraries. A common problem with fuzzing is defining the expected behaviour of a program; this is known as the test oracle problem \cite{fuzzing}.

\subsubsection{Fuzzing Classification}
Black box fuzzing does not require the internals of the software system to be exposed to the fuzzer \cite{fuzzing}. In contrast white box fuzzing, which requires detailed knowledge of the internals of the system being tested. We can utilize white box fuzzing techniques since both the parser combinator API and the structure of arbitrary parsers are strongly typed.

\subsubsection{Sample Generation}
Several techniques of sample generation have been explored in the context of parsers. 

\citeauthor{parser-directed} explores parser-directed fuzzing by using dynamic tainting to resolve parse rejections. This approach relies on a direct taint, which is often ineffective during tokenization stages of parsers when the data flow is broken, limiting the exploration of the input space.

Symbolic execution for fuzzing tackles the sample generation challenge by using constraint solving to increases statement coverage \cite{klee}. Path explosion is particularly problematic in the context of parsers as paths predominantly cause parse rejections \cite{path-explosion}. Samples that are accepted by the parser remain highly improbable \cite{parser-directed}.

Grammar-based fuzzing, which involves generating samples directly from a formal grammar, avoids much of the trial and error of random input generation for parsers \cite{grammar}. Traditionally, grammars are difficult to write and maintain, often do not reflect the actual grammar implemented by a parser, and are not necessarily machine readable.

\subsubsection{Differential Testing}
Gigaparsec and Parsley are intended to be API compatible, but have significantly different implementations due to the differences in the host languages (Haskell and Scale respectively). Consequently, it seems plausible that there would be different sets of bugs in each library, leaving opportunity for differential testing. Parsley is also much more mature that Gigaparsec, meaning it could be used as a reference to support the stability of Gigaparsec throughout its early development and optimisation.

Therefore, the advantages of differential testing are that it seems promising in catching implementation errors, however, the disadvantage is that more conceptual mistakes may be repeated in both libraries and not caught when comparing between the two. For example, an incorrect optimisation step might be applied in both Gigaparsec and Parsley causing the incorrect behaviour to be consistent and thus not be caught by differential testing.

\subsubsection{QuickSpec}

QuickSpec is a tool for automated algebraic specification generation \cite{quickspec}. It could be applicable here to quickly extend the set of parser combinator laws. The generated laws can help to identify bugs when recognised as invalid and help extend our laws to better understand the behaviour of parser combinators.

One disadvantage of this tool is that it relies on randomised testing, occasionally invalid laws may be discovered simply because a counter counter example is not found, which is not necessarily indicative of a library bug. Another disadvantage is that the process is not automated; generated laws need to be manually inspected to validate or recognise issues.

\subsubsection{Metamorphic Testing}

Metamorphic testing is a form of fuzz testing that involves testing a specified relationship between two versions of a program \cite{metamorphic}. The parser laws could be leveraged as initial metamorphic relations, however, these laws are defined in terms of primitive combinators, therefore it might be harder to detect bugs in combinators at higher levels of abstraction. To extend the implementation to cover more abstract combinators, metamorphic relations using these combinator could be defined.

A significant advantage of metamorphic fuzzing is that we are able to quickly reduce a test case to a minimal example by reducing the metamorphic transformations to a minimal sequence \cite{deduplication}. Writing these reducer may be left to future work, however, even inspecting the metamorphic transformations make bug identification much easier.

\subsubsection{QuickCheck}

QuickCheck is a Haskell library for property based testing \cite{quickcheck}. It allows properties to be specified as Haskell function, which are then tested with randomly generated inputs. It has support for custom data generators, monitoring of sample distribution, and infinite structures. 

\subsection{Summary}

We have discussed how various fuzzing techniques apply considering the context of parser combinator libraries. We narrowed to a white box fuzzing approach to leverage our unique knowledge of the parser structure, which follows the structure of the target grammar. This is uniquely advantageous for solving the sample generation problem. Considering differential fuzzing, we determined that there is too high potential for bug duplication across Parsley and Gigaparsec. Considering QuickSpec for formal specification generation, we found that the likelihood of false positives would require excessive manual verification. Finally, we determined that metamorphic testing is a promising approach to build on the existing knowledge of parser combinator laws and easily reduce test cases. Further investigation into the most effective metamorphic relations may be required to scale the solution beyond this project.

\section{Evaluation Plan}

The project is evaluated using several metrics. It is important that experiments are carried out carefully using multiple trials, statistical tests, and considering a variety of random seeds to avoid result bias \cite{evaluation}.

\subsection{Unique Bugs}

When evaluating the total bugs discovered during the experiments, it is important that duplication is considered. For example, a bug discovered in one parser, may also be discovered in similar parsers.

Deduplication via reduction of the generated parsers and test samples would be desirable to report as many distinct bugs as possible between versions \cite{deduplication}. However, this is a non-trivial problem and is outside the scope of the project. Some bug deduplication may be possible manually, otherwise, over counting will be a limitation of this metric in the evaluation.


\subsection{Bug Injection}

Existing fuzzing tools cannot be used for comparison since the generation of arbitrary parsers are entirely custom. Regardless, evaluating the ability of the tool to discover injected bugs will allow us to validate the thoroughness of the fuzzing approach. We can use old bugs or mutation when injecting new bugs.

\subsection{Statement Coverage}

Statement coverage is a common evaluation metric for fuzzing. \citeauthor{coverage} argue that statement coverage is a poor indicator of test suite effectiveness and show a low correlation. Statement coverage does not necessarily account for the complexity of the interactions between statements, which may be especially true for parser combinators, where the interactions between combinators can be intricate. For these reasons, statement coverage may not be a primary evaluation metric for this project.

\subsection{Practical Significance of Bugs}

We hypothesize that any Gigaparsec bugs discovered have a high likelihood of being relevant in practical applications. \citeauthor{fuzzing-importance} demonstrates that similar is true for compiler fuzzing.

However, it could still be argued that many bugs discovered in this project are niche cases and unlikely to be found through realistic use of the library. A formal evaluation of the practical significance of any bugs discovered would be interesting to explore, but this may require the analysis of many real-world Gigaparsec parsers and is thus outside of the scope of the project.

\subsection{Qualitative Measures}

Qualitatively, the implementation should be well integrated into the Gigaparsec library, easy to use with future version, be well documented, and have detailed execution logs. The implementation may also benefit from continuous integration if found to be effective.

\section{Project Plan}

As for my personal schedule, I am working full-time as a Software Engineer during the project and will have consistent free time (or lack thereof) from month to month. As a result, the project workload is distributed evenly.

\subsection{Milestone 1: Parser Generation}
\be{February, 2024}

The first goal is to implement generators for some of the primitive combinators in Gigaparsec using QuickCheck \cite{quickcheck}. Some progress has been made towards this goal, but more work is required to support additional generators, implement size bounds for the generated parsers, and the consider the distribution of generated parsers.

\subsection{Milestone 2: Input Generation}
\be{March, 2024}

The second milestone is to implement input generation for the parsers themselves by inspecting the structure of arbitrary parsers. High coverage of the input space is important here, ideally with many inputs being accepted by the parser to test both acceptance and rejection states.

\subsection{Milestone 3: Metamorphic Testing}
\be{April, 2024}

In the third milestone, parser combinators laws will be used to find metamorphic relations for generated parsers, which are then tested against each other to check adherence to the laws. Initially equivalence relations can be used, but other relations may be explored if time allows. Multiple transformations can also be applied per test to find more complex interactions, although it is worth noting that this will complicate test case deduplication.

\subsection{Milestone 4: Experiments, Evaluation, and Report}
\be{June 17, 2024}

This milestone has been allocated the largest amount of time, partially as a buffer for the previous milestones, and partially as there is some uncertainty surrounding the time required for the final milestone. Both the execution time for the fuzz testing and and the hardware available is unclear. It may also be necessary to implement test case reduction in this stage to help with the interpretation of the results. The final report will be written in parallel alongside running the experiments for this milestone.

\subsection{Extensions}

If time permits, there are a several areas for extension. Both the combinators and combinator laws could be extended to test Gigaparsec more thoroughly, alternatively, other fuzzing techniques could be explored to evaluate their potential for future research. Differential testing of Gigaparsec against Parsley is particularly interesting due to the drastic difference in host languages.

\subsection{Fallback Plan}
Each milestone is designed to be flexible in its totality, allowing for easy adaption to meet project deadlines. For example, in the first milestone, the number of supported combinators is variable. The second milestone is the least flexible; good coverage of the input space is required for the supported parsers. For the third milestone, the parser laws tested can be reduced. For the final milestone, the execution time for the experiments can be reduced at the cost of reduced thoroughness.

\raggedright
\bibliography{interim-report.bib}
\end{document}

% Similarly to other successful parser-direct fuzzing approach, the parser must be exposed to the fuzzer for our implementation.